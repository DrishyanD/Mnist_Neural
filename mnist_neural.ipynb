{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4c19272a-e9b6-4944-94c4-9c55fc5cc45a",
   "metadata": {},
   "source": [
    "# Handwritten Digit Recognition Using a 2-Layer Neural Network\n",
    "\n",
    "This project builds a **simple neural network** from scratch to recognize handwritten digits (0-9).  \n",
    "Here’s a **step-by-step breakdown** of what happens, in plain language.\n",
    "\n",
    "---\n",
    "\n",
    "## 1. Preparing the Data\n",
    "- Each image is a 28×28 grid of pixels (total 784 numbers).  \n",
    "- We **flatten** each image into a single column of 784 numbers so the network can process it.  \n",
    "- Pixel values range from 0–255. We **divide by 255** so all values are between 0 and 1. This makes learning faster and more stable.  \n",
    "- Labels (the correct digit) are stored separately.  \n",
    "- Training data is **shuffled** so the network doesn’t learn patterns from the order of the examples.  \n",
    "\n",
    "---\n",
    "\n",
    "## 2. Initializing the Network\n",
    "- The network has **2 layers**:\n",
    "  1. Hidden layer with 10 neurons  \n",
    "  2. Output layer with 10 neurons (one for each digit)  \n",
    "- **Weights (W1, W2)** are numbers that decide how strongly each input affects a neuron.  \n",
    "- **Biases (b1, b2)** allow neurons to turn on even if the input is zero.  \n",
    "- Both weights and biases are initialized randomly so that neurons start differently and can learn unique features.\n",
    "\n",
    "---\n",
    "\n",
    "## 3. Forward Propagation (Making Predictions)\n",
    "- The input image goes through the network step by step:\n",
    "  1. **Hidden layer**: Multiply input by weights, add bias → gives Z1.  \n",
    "     - Apply **ReLU**: set negative numbers to 0 → gives A1.  \n",
    "     - ReLU lets the network learn complex patterns.\n",
    "  2. **Output layer**: Multiply hidden layer outputs by W2, add b2 → gives Z2.  \n",
    "     - Apply **Softmax**: converts raw scores into probabilities → gives A2.  \n",
    "     - Each neuron now has a probability for the image being that digit.  \n",
    "\n",
    "---\n",
    "\n",
    "## 4. Calculating Loss\n",
    "- **Loss** measures how wrong the network is.  \n",
    "- We use **cross-entropy loss**, which looks at the probability of the correct label.  \n",
    "- Lower loss = better predictions.\n",
    "\n",
    "---\n",
    "\n",
    "## 5. Backward Propagation (Learning from Mistakes)\n",
    "- The network calculates **gradients**: how much each weight and bias contributed to the error.  \n",
    "- ReLU derivative is used to make sure only active neurons pass gradients backward.  \n",
    "- Gradients tell the network which weights and biases to adjust and by how much.\n",
    "\n",
    "---\n",
    "\n",
    "## 6. Updating Parameters\n",
    "- Use **gradient descent**:\n",
    "  - Subtract (gradient × learning rate) from each weight and bias.  \n",
    "  - This makes the network slightly better at predicting each step.  \n",
    "- Repeat **forward → loss → backward → update** many times (iterations) until the network learns well.\n",
    "\n",
    "---\n",
    "\n",
    "## 7. Making Predictions\n",
    "- After training, the network can predict digits:\n",
    "  - Feed an image → forward propagation → output probabilities → choose the digit with the highest probability.  \n",
    "- Can check predictions on individual images or the full dataset.\n",
    "\n",
    "---\n",
    "\n",
    "## 8. Evaluating Performance\n",
    "- Compare predictions to true labels:\n",
    "  - **Accuracy** = percentage of correct predictions.  \n",
    "- Can visualize a few images with their predicted and true labels to see if the network is working as expected.\n",
    "\n",
    "---\n",
    "\n",
    "## Summary in Simple Terms\n",
    "1. **Look at the image → turn pixels into numbers**  \n",
    "2. **Mix numbers with weights and add biases → see which neurons “fire”**  \n",
    "3. **Use hidden neurons to combine signals → produce output probabilities**  \n",
    "4. **Check how wrong the prediction is (loss)**  \n",
    "5. **Send error backwards → tweak weights and biases**  \n",
    "6. **Repeat many times → network gets better**  \n",
    "7. **Use the trained network to predict new images**  \n",
    "\n",
    "This is how a neural network “learns” from data, step by step, even without using high-level deep learning libraries.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc0539e5-f765-4707-b243-0d87db45e61b",
   "metadata": {},
   "source": [
    "# Key Concepts and Definitions\n",
    "\n",
    "Here are the main terms used in the neural network, explained in simple language:\n",
    "\n",
    "---\n",
    "\n",
    "## **Neuron**\n",
    "- A single processing unit in the network.\n",
    "- Takes input numbers, combines them, and decides whether to “fire” (send a signal) to the next layer.\n",
    "\n",
    "---\n",
    "\n",
    "## **Weights (W1, W2)**\n",
    "- Numbers that control how important each input is for a neuron.\n",
    "- Bigger weight → input has more influence; smaller weight → input has less influence.\n",
    "\n",
    "---\n",
    "\n",
    "## **Biases (b1, b2)**\n",
    "- Extra numbers added to the weighted sum before activation.\n",
    "- Allow a neuron to activate even if the input is zero.\n",
    "\n",
    "---\n",
    "\n",
    "## **Z1, Z2 (Linear Combinations)**\n",
    "- The result of multiplying inputs by weights and adding biases.\n",
    "- Raw values before activation functions.\n",
    "\n",
    "---\n",
    "\n",
    "## **Activation Function**\n",
    "- A rule that decides the output of a neuron.\n",
    "- Makes the network capable of learning complex, non-linear patterns.\n",
    "\n",
    "### **ReLU (Rectified Linear Unit)**\n",
    "- Activation function for the hidden layer.\n",
    "- Sets negative numbers to 0 and keeps positive numbers as they are.\n",
    "- Helps the network learn important features.\n",
    "\n",
    "### **Softmax**\n",
    "- Activation function for the output layer.\n",
    "- Turns raw scores into probabilities.\n",
    "- Each output neuron represents the probability that the input belongs to a certain class (0-9).\n",
    "- All probabilities add up to 1.\n",
    "\n",
    "---\n",
    "\n",
    "## **Forward Propagation**\n",
    "- Process of passing input through the network to get predictions.\n",
    "- Input → weighted sum → activation → output probabilities.\n",
    "\n",
    "---\n",
    "\n",
    "## **Loss**\n",
    "- A number that tells us how wrong the network is.\n",
    "- Lower loss = better predictions.\n",
    "- We use **cross-entropy loss**, which looks at the probability assigned to the correct label.\n",
    "\n",
    "---\n",
    "\n",
    "## **Backward Propagation**\n",
    "- Process of sending the error back through the network.\n",
    "- Calculates how much each weight and bias contributed to the error.\n",
    "- Helps the network learn by adjusting weights and biases.\n",
    "\n",
    "---\n",
    "\n",
    "## **Gradient**\n",
    "- Tells us **how much to change a weight or bias** to make predictions better.\n",
    "- Calculated during backward propagation.\n",
    "\n",
    "---\n",
    "\n",
    "## **Gradient Descent**\n",
    "- A method to **update weights and biases** using gradients.\n",
    "- Learning rate controls **how big each update step is**.\n",
    "- Repeat many times until the network learns well.\n",
    "\n",
    "---\n",
    "\n",
    "## **Prediction**\n",
    "- After forward propagation, choose the class (digit) with the **highest probability** as the network’s answer.\n",
    "\n",
    "---\n",
    "\n",
    "## **Accuracy**\n",
    "- Percentage of predictions that match the true labels.\n",
    "- Measures how well the network is performing.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2bbbb18d-3f17-44a8-825e-68252b4c3e81",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing Libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "323ab9fd-de64-4940-882a-9d8d456e6fd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting data to train and test\n",
    "data_train = pd.read_csv(\"./data/mnist_train.csv\",header=None)\n",
    "data_test = pd.read_csv(\"./data/mnist_test.csv\",header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cdfa4a1d-d39f-4439-b4c7-b515b57327cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>775</th>\n",
       "      <th>776</th>\n",
       "      <th>777</th>\n",
       "      <th>778</th>\n",
       "      <th>779</th>\n",
       "      <th>780</th>\n",
       "      <th>781</th>\n",
       "      <th>782</th>\n",
       "      <th>783</th>\n",
       "      <th>784</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 785 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   0    1    2    3    4    5    6    7    8    9    ...  775  776  777  778  \\\n",
       "0    5    0    0    0    0    0    0    0    0    0  ...    0    0    0    0   \n",
       "1    0    0    0    0    0    0    0    0    0    0  ...    0    0    0    0   \n",
       "2    4    0    0    0    0    0    0    0    0    0  ...    0    0    0    0   \n",
       "3    1    0    0    0    0    0    0    0    0    0  ...    0    0    0    0   \n",
       "4    9    0    0    0    0    0    0    0    0    0  ...    0    0    0    0   \n",
       "\n",
       "   779  780  781  782  783  784  \n",
       "0    0    0    0    0    0    0  \n",
       "1    0    0    0    0    0    0  \n",
       "2    0    0    0    0    0    0  \n",
       "3    0    0    0    0    0    0  \n",
       "4    0    0    0    0    0    0  \n",
       "\n",
       "[5 rows x 785 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "648a7538-0a51-46fe-8384-c23065696956",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>775</th>\n",
       "      <th>776</th>\n",
       "      <th>777</th>\n",
       "      <th>778</th>\n",
       "      <th>779</th>\n",
       "      <th>780</th>\n",
       "      <th>781</th>\n",
       "      <th>782</th>\n",
       "      <th>783</th>\n",
       "      <th>784</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 785 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   0    1    2    3    4    5    6    7    8    9    ...  775  776  777  778  \\\n",
       "0    7    0    0    0    0    0    0    0    0    0  ...    0    0    0    0   \n",
       "1    2    0    0    0    0    0    0    0    0    0  ...    0    0    0    0   \n",
       "2    1    0    0    0    0    0    0    0    0    0  ...    0    0    0    0   \n",
       "3    0    0    0    0    0    0    0    0    0    0  ...    0    0    0    0   \n",
       "4    4    0    0    0    0    0    0    0    0    0  ...    0    0    0    0   \n",
       "\n",
       "   779  780  781  782  783  784  \n",
       "0    0    0    0    0    0    0  \n",
       "1    0    0    0    0    0    0  \n",
       "2    0    0    0    0    0    0  \n",
       "3    0    0    0    0    0    0  \n",
       "4    0    0    0    0    0    0  \n",
       "\n",
       "[5 rows x 785 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a0959cda-0f00-429b-9c95-1b3d0a15dcdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert training and testing data into NumPy arrays\n",
    "# This makes the data easier and faster to work with\n",
    "train_data = np.array(data_train)\n",
    "test_data = np.array(data_test)\n",
    "\n",
    "# Shuffle the training data randomly\n",
    "# This helps prevent the model from learning patterns based on data order\n",
    "np.random.shuffle(train_data)\n",
    "\n",
    "# Separate labels and features for training data\n",
    "# The first column contains the labels (Y)\n",
    "# The remaining columns contain pixel values (X)\n",
    "Y_train = train_data[:, 0]\n",
    "X_train = train_data[:, 1:].T / 255  # Transpose and normalize pixel values\n",
    "\n",
    "# Separate labels and features for development (test) data\n",
    "Y_dev = test_data[:, 0]\n",
    "X_dev = test_data[:, 1:].T / 255  # Transpose and normalize pixel values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4311a9aa-d5f1-42c3-8684-8cb594dfe52d",
   "metadata": {},
   "source": [
    "First, we convert our training and testing data into NumPy arrays so that we can perform mathematical operations on them efficiently. NumPy is much faster and more convenient for handling large datasets like images. \n",
    "\n",
    "Next, we shuffle the training data. This is important because it ensures that the model does not learn based on the order of the data, which could cause biased or incorrect learning.\n",
    "\n",
    "Then, we split the data into labels and features. The labels (Y_train and Y_dev) are stored in the first column and represent the correct answers (for example, the digit in a digit-recognition dataset). The features (X_train and X_dev) are the pixel values of the images.\n",
    "\n",
    "Finally, we normalize the pixel values by dividing by 255 so that all values are between 0 and 1. This makes training the model more stable and efficient. We also transpose the feature matrix so that each column represents one training example, which is the format many neural networks expect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "16b3b4ae-2916-4133-9549-b2836beef3e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(784, 60000) (60000,)\n",
      "(784, 10000) (10000,)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape, Y_train.shape)\n",
    "print(X_dev.shape, Y_dev.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0a4f9a5-cd26-4e0f-8033-4741c8911fbc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bf009a6c-f1bd-4fcf-8c07-dee670065253",
   "metadata": {},
   "outputs": [],
   "source": [
    "## \n",
    "def init_params():\n",
    "    W1 = np.random.rand(10, 784) - 0.5\n",
    "    b1 = np.random.rand(10, 1) - 0.5\n",
    "    W2 = np.random.rand(10, 10) - 0.5\n",
    "    b2 = np.random.rand(10,1) - 0.5\n",
    "    return W1, b1, W2, b2\n",
    "\n",
    "def ReLU(Z):\n",
    "    return np.maximum(Z,0)\n",
    "\n",
    "def softmax(Z):\n",
    "    expZ = np.exp(Z - np.max(Z, axis=0, keepdims=True))\n",
    "    return expZ / np.sum(expZ, axis=0, keepdims=True)\n",
    "\n",
    "\n",
    "def forward_prop(W1, b1, W2, b2, X):\n",
    "    Z1 = W1.dot(X) + b1\n",
    "    A1 = ReLU(Z1)\n",
    "    Z2 = W2.dot(A1) + b2\n",
    "    A2 = softmax(Z2)\n",
    "    return Z1, A1, Z2, A2\n",
    "\n",
    "def ReLU_deriv(Z):\n",
    "    return Z > 0\n",
    "\n",
    "def one_hot(Y):\n",
    "    one_hot_Y = np.zeros((Y.size, Y.max() + 1))\n",
    "    one_hot_Y[np.arange(Y.size), Y] = 1\n",
    "    one_hot_Y = one_hot_Y.T\n",
    "    return one_hot_Y\n",
    "\n",
    "\n",
    "def backward_prop(Z1, A1, Z2, A2, W2, X, Y):\n",
    "    m = Y.size\n",
    "    one_hot_Y = one_hot(Y)\n",
    "    dZ2 = A2 - one_hot_Y\n",
    "    dW2 = 1 / m * dZ2.dot(A1.T)\n",
    "    db2 = 1 / m * np.sum(dZ2, axis=1, keepdims=True)\n",
    "    dZ1 = W2.T.dot(dZ2) * ReLU_deriv(Z1)\n",
    "    dW1 = 1 / m * dZ1.dot(X.T)\n",
    "    db1 = 1 / m * np.sum(dZ1, axis=1, keepdims=True)\n",
    "    return dW1, db1, dW2, db2\n",
    "\n",
    "\n",
    "def update_params(W1, b1, W2, b2, dW1, db1, dW2, db2, alpha):\n",
    "    W1 -= alpha * dW1\n",
    "    b1 -= alpha * db1\n",
    "    W2 -= alpha * dW2\n",
    "    b2 -= alpha * db2\n",
    "    return W1, b1, W2, b2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "174f5526-dd8a-4c17-b932-26a431f69ada",
   "metadata": {},
   "source": [
    "# Neural Network Helper Functions – Explanation\n",
    "\n",
    "This section defines all the components used to build and train a simple **2-layer neural network** for image classification (for example, handwritten digits).\n",
    "\n",
    "### Network Structure\n",
    "1. **Input layer:** 784 neurons (28 × 28 pixel images)\n",
    "2. **Hidden layer:** 10 neurons (ReLU activation)\n",
    "3. **Output layer:** 10 neurons (Softmax activation)\n",
    "\n",
    "---\n",
    "\n",
    "## X (Input Data)\n",
    "1. Represents the input images.\n",
    "2. Each image is converted into **784 numbers** (28 × 28 pixels).\n",
    "3. Shape: **(784, number_of_examples)**.\n",
    "4. These values are the pixel intensities used by the model to make predictions.\n",
    "\n",
    "---\n",
    "\n",
    "## Y (Labels)\n",
    "1. Contains the correct answers for each image.\n",
    "2. Each value represents a digit from **0 to 9**.\n",
    "3. Used to measure how accurate the model’s predictions are.\n",
    "\n",
    "---\n",
    "\n",
    "## W1 (Weights of First Layer)\n",
    "1. Connects the input layer to the hidden layer.\n",
    "2. Shape: **(10, 784)**.\n",
    "3. Each weight represents the importance of a specific pixel for a hidden neuron.\n",
    "4. Helps the model learn basic patterns like edges and shapes.\n",
    "\n",
    "---\n",
    "\n",
    "## b1 (Bias of First Layer)\n",
    "1. Added to the weighted sum before applying the activation function.\n",
    "2. Shape: **(10, 1)**.\n",
    "3. Allows neurons to activate even when input values are small.\n",
    "4. Helps shift the activation function for better learning.\n",
    "\n",
    "---\n",
    "\n",
    "## Z1 (Linear Combination – First Layer)\n",
    "1. Result of multiplying input data by `W1` and adding `b1`.\n",
    "2. Represents raw values before applying the activation function.\n",
    "3. Shape: **(10, number_of_examples)**.\n",
    "\n",
    "---\n",
    "\n",
    "## A1 (Activation of Hidden Layer)\n",
    "1. Output after applying the **ReLU** activation function to `Z1`.\n",
    "2. Sets negative values to zero and keeps positive values.\n",
    "3. Introduces non-linearity, allowing the network to learn complex patterns.\n",
    "4. Shape: **(10, number_of_examples)**.\n",
    "\n",
    "---\n",
    "\n",
    "## W2 (Weights of Second Layer)\n",
    "1. Connects the hidden layer to the output layer.\n",
    "2. Shape: **(10, 10)**.\n",
    "3. Controls how much each hidden neuron influences the final output.\n",
    "\n",
    "---\n",
    "\n",
    "## b2 (Bias of Second Layer)\n",
    "1. Added to the weighted sum before applying the softmax function.\n",
    "2. Shape: **(10, 1)**.\n",
    "3. Helps adjust the final output values.\n",
    "\n",
    "---\n",
    "\n",
    "## Z2 (Linear Combination – Output Layer)\n",
    "1. Result of multiplying `A1` by `W2` and adding `b2`.\n",
    "2. Represents raw scores for each class (digits 0–9).\n",
    "3. Shape: **(10, number_of_examples)**.\n",
    "\n",
    "---\n",
    "\n",
    "## A2 (Final Output / Predictions)\n",
    "1. Output after applying the **softmax** function to `Z2`.\n",
    "2. Converts raw scores into probabilities.\n",
    "3. Each column sums to **1**.\n",
    "4. The index with the highest probability is the predicted digit.\n",
    "\n",
    "---\n",
    "\n",
    "## one_hot_Y (One-Hot Encoded Labels)\n",
    "1. Converts labels into binary vectors.\n",
    "2. Used to compare predictions with the true labels.\n",
    "3. Makes loss and gradient calculations easier.\n",
    "\n",
    "---\n",
    "\n",
    "## dW1, db1, dW2, db2 (Gradients)\n",
    "1. Represent how much each weight and bias affects the model’s error.\n",
    "2. Computed during backpropagation.\n",
    "3. Used to update parameters and improve model performance.\n",
    "\n",
    "---\n",
    "\n",
    "## α (Learning Rate)\n",
    "1. Controls how much the weights and biases are updated each step.\n",
    "2. Small values lead to slow learning.\n",
    "3. Large values may cause unstable learning.\n",
    "\n",
    "---\n",
    "\n",
    "## m (Number of Training Examples)\n",
    "1. Represents the total number of samples in the dataset.\n",
    "2. Used to average gradients for stable parameter updates.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bbfa0fb3-94ee-4175-9ac8-112d28883e0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_predictions(A2):\n",
    "    return np.argmax(A2, axis=0)\n",
    "\n",
    "\n",
    "def get_accuracy(predictions, Y):\n",
    "    return np.mean(predictions == Y)\n",
    "\n",
    "def compute_loss(A2, Y):\n",
    "    m = Y.size\n",
    "    log_probs = -np.log(A2[Y, np.arange(m)] + 1e-8)\n",
    "    return np.sum(log_probs) / m\n",
    "\n",
    "\n",
    "def gradient_descent(X, Y, iterations, alpha):\n",
    "    W1, b1, W2, b2 = init_params()\n",
    "\n",
    "    for i in range(iterations):\n",
    "        Z1, A1, Z2, A2 = forward_prop(W1, b1, W2, b2, X)\n",
    "        dW1, db1, dW2, db2 = backward_prop(Z1, A1, Z2, A2, W2, X, Y)\n",
    "        W1, b1, W2, b2 = update_params(\n",
    "            W1, b1, W2, b2, dW1, db1, dW2, db2, alpha\n",
    "        )\n",
    "\n",
    "        if i % 10 == 0:\n",
    "            predictions = get_predictions(A2)\n",
    "            acc = get_accuracy(predictions, Y)\n",
    "            loss = compute_loss(A2, Y)\n",
    "            print(f\"Iter {i:4d} | Loss: {loss:.4f} | Acc: {acc:.4f}\")\n",
    "\n",
    "    return W1, b1, W2, b2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "388a4d4c-eec1-49bd-b226-1f874527ed1e",
   "metadata": {},
   "source": [
    "# Training and Evaluation Functions – Explanation\n",
    "\n",
    "This section defines helper functions used to **evaluate predictions**, **measure model performance**, **compute loss**, and **train the neural network using gradient descent**.\n",
    "\n",
    "---\n",
    "\n",
    "## get_predictions(A2)\n",
    "1. Converts the output probabilities into predicted class labels.\n",
    "2. Selects the index with the highest probability for each example.\n",
    "3. Uses `argmax` along axis 0.\n",
    "4. Output shape: **(number_of_examples, )**.\n",
    "\n",
    "---\n",
    "\n",
    "## predictions\n",
    "1. Stores the predicted class labels.\n",
    "2. Each value is a digit from **0 to 9**.\n",
    "3. Used to evaluate model accuracy.\n",
    "\n",
    "---\n",
    "\n",
    "## get_accuracy(predictions, Y)\n",
    "1. Compares predicted labels with the true labels.\n",
    "2. Counts how many predictions are correct.\n",
    "3. Returns the average accuracy as a value between **0 and 1**.\n",
    "4. Used to track how well the model is performing.\n",
    "\n",
    "---\n",
    "\n",
    "## compute_loss(A2, Y)\n",
    "1. Calculates the **cross-entropy loss**.\n",
    "2. Measures how far the predicted probabilities are from the true labels.\n",
    "3. Uses only the probability of the correct class for each example.\n",
    "4. A small value (1e-8) is added to avoid taking the log of zero.\n",
    "5. Lower loss means better model performance.\n",
    "\n",
    "---\n",
    "\n",
    "## loss\n",
    "1. Represents the average error of the model.\n",
    "2. Used to monitor learning progress during training.\n",
    "3. Should decrease as training continues.\n",
    "\n",
    "---\n",
    "\n",
    "## gradient_descent(X, Y, iterations, alpha)\n",
    "1. Trains the neural network using **gradient descent**.\n",
    "2. Repeatedly updates weights and biases to reduce loss.\n",
    "3. Returns the trained parameters.\n",
    "\n",
    "---\n",
    "\n",
    "## iterations\n",
    "1. Number of times the training loop runs.\n",
    "2. More iterations allow the model to learn better.\n",
    "3. Too many iterations may cause overfitting.\n",
    "\n",
    "---\n",
    "\n",
    "## alpha (Learning Rate)\n",
    "1. Controls how much parameters are updated each step.\n",
    "2. Small values lead to slow learning.\n",
    "3. Large values may cause unstable training.\n",
    "\n",
    "---\n",
    "\n",
    "## Training Loop (Inside Gradient Descent)\n",
    "1. Initializes weights and biases.\n",
    "2. Performs **forward propagation** to get predictions.\n",
    "3. Uses **backward propagation** to compute gradients.\n",
    "4. Updates parameters using gradient descent.\n",
    "5. Repeats this process for the given number of iterations.\n",
    "\n",
    "---\n",
    "\n",
    "## Printing Progress\n",
    "1. Every 10 iterations, the model prints:\n",
    "   - Current iteration number\n",
    "   - Loss value\n",
    "   - Accuracy value\n",
    "2. Helps monitor training performance over time.\n",
    "\n",
    "---\n",
    "\n",
    "## Returned Values\n",
    "1. `W1, b1, W2, b2` are returned after training.\n",
    "2. These represent the learned parameters of the model.\n",
    "3. Used later for making predictions on new data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f0f92847-b5f7-4717-a3da-851105b993ec",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter    0 | Loss: 3.5637 | Acc: 0.0949\n",
      "Iter   10 | Loss: 2.2190 | Acc: 0.1860\n",
      "Iter   20 | Loss: 2.0569 | Acc: 0.2580\n",
      "Iter   30 | Loss: 1.9365 | Acc: 0.3013\n",
      "Iter   40 | Loss: 1.8214 | Acc: 0.3424\n",
      "Iter   50 | Loss: 1.7110 | Acc: 0.3841\n",
      "Iter   60 | Loss: 1.6081 | Acc: 0.4225\n",
      "Iter   70 | Loss: 1.5144 | Acc: 0.4604\n",
      "Iter   80 | Loss: 1.4276 | Acc: 0.4971\n",
      "Iter   90 | Loss: 1.3456 | Acc: 0.5323\n",
      "Iter  100 | Loss: 1.2675 | Acc: 0.5643\n",
      "Iter  110 | Loss: 1.1950 | Acc: 0.5930\n",
      "Iter  120 | Loss: 1.1301 | Acc: 0.6182\n",
      "Iter  130 | Loss: 1.0733 | Acc: 0.6397\n",
      "Iter  140 | Loss: 1.0240 | Acc: 0.6574\n",
      "Iter  150 | Loss: 0.9813 | Acc: 0.6722\n",
      "Iter  160 | Loss: 0.9441 | Acc: 0.6848\n",
      "Iter  170 | Loss: 0.9114 | Acc: 0.6970\n",
      "Iter  180 | Loss: 0.8822 | Acc: 0.7076\n",
      "Iter  190 | Loss: 0.8558 | Acc: 0.7182\n",
      "Iter  200 | Loss: 0.8317 | Acc: 0.7272\n",
      "Iter  210 | Loss: 0.8095 | Acc: 0.7363\n",
      "Iter  220 | Loss: 0.7887 | Acc: 0.7443\n",
      "Iter  230 | Loss: 0.7693 | Acc: 0.7528\n",
      "Iter  240 | Loss: 0.7510 | Acc: 0.7595\n",
      "Iter  250 | Loss: 0.7338 | Acc: 0.7663\n",
      "Iter  260 | Loss: 0.7173 | Acc: 0.7728\n",
      "Iter  270 | Loss: 0.7016 | Acc: 0.7783\n",
      "Iter  280 | Loss: 0.6866 | Acc: 0.7839\n",
      "Iter  290 | Loss: 0.6724 | Acc: 0.7892\n",
      "Iter  300 | Loss: 0.6589 | Acc: 0.7943\n",
      "Iter  310 | Loss: 0.6460 | Acc: 0.7991\n",
      "Iter  320 | Loss: 0.6339 | Acc: 0.8030\n",
      "Iter  330 | Loss: 0.6224 | Acc: 0.8076\n",
      "Iter  340 | Loss: 0.6115 | Acc: 0.8117\n",
      "Iter  350 | Loss: 0.6012 | Acc: 0.8154\n",
      "Iter  360 | Loss: 0.5915 | Acc: 0.8189\n",
      "Iter  370 | Loss: 0.5824 | Acc: 0.8218\n",
      "Iter  380 | Loss: 0.5737 | Acc: 0.8250\n",
      "Iter  390 | Loss: 0.5655 | Acc: 0.8277\n",
      "Iter  400 | Loss: 0.5578 | Acc: 0.8303\n",
      "Iter  410 | Loss: 0.5505 | Acc: 0.8327\n",
      "Iter  420 | Loss: 0.5436 | Acc: 0.8352\n",
      "Iter  430 | Loss: 0.5371 | Acc: 0.8374\n",
      "Iter  440 | Loss: 0.5310 | Acc: 0.8398\n",
      "Iter  450 | Loss: 0.5252 | Acc: 0.8417\n",
      "Iter  460 | Loss: 0.5197 | Acc: 0.8435\n",
      "Iter  470 | Loss: 0.5145 | Acc: 0.8454\n",
      "Iter  480 | Loss: 0.5095 | Acc: 0.8472\n",
      "Iter  490 | Loss: 0.5047 | Acc: 0.8489\n",
      "Iter  500 | Loss: 0.5002 | Acc: 0.8506\n",
      "Iter  510 | Loss: 0.4958 | Acc: 0.8523\n",
      "Iter  520 | Loss: 0.4917 | Acc: 0.8539\n",
      "Iter  530 | Loss: 0.4877 | Acc: 0.8553\n",
      "Iter  540 | Loss: 0.4839 | Acc: 0.8563\n",
      "Iter  550 | Loss: 0.4802 | Acc: 0.8575\n",
      "Iter  560 | Loss: 0.4767 | Acc: 0.8588\n",
      "Iter  570 | Loss: 0.4733 | Acc: 0.8599\n",
      "Iter  580 | Loss: 0.4701 | Acc: 0.8610\n",
      "Iter  590 | Loss: 0.4669 | Acc: 0.8621\n",
      "Iter  600 | Loss: 0.4639 | Acc: 0.8632\n",
      "Iter  610 | Loss: 0.4609 | Acc: 0.8642\n",
      "Iter  620 | Loss: 0.4581 | Acc: 0.8650\n",
      "Iter  630 | Loss: 0.4554 | Acc: 0.8658\n",
      "Iter  640 | Loss: 0.4527 | Acc: 0.8669\n",
      "Iter  650 | Loss: 0.4502 | Acc: 0.8676\n",
      "Iter  660 | Loss: 0.4477 | Acc: 0.8686\n",
      "Iter  670 | Loss: 0.4453 | Acc: 0.8692\n",
      "Iter  680 | Loss: 0.4429 | Acc: 0.8701\n",
      "Iter  690 | Loss: 0.4407 | Acc: 0.8708\n",
      "Iter  700 | Loss: 0.4385 | Acc: 0.8716\n",
      "Iter  710 | Loss: 0.4363 | Acc: 0.8724\n",
      "Iter  720 | Loss: 0.4342 | Acc: 0.8729\n",
      "Iter  730 | Loss: 0.4322 | Acc: 0.8734\n",
      "Iter  740 | Loss: 0.4302 | Acc: 0.8738\n",
      "Iter  750 | Loss: 0.4283 | Acc: 0.8745\n",
      "Iter  760 | Loss: 0.4265 | Acc: 0.8750\n",
      "Iter  770 | Loss: 0.4246 | Acc: 0.8758\n",
      "Iter  780 | Loss: 0.4228 | Acc: 0.8765\n",
      "Iter  790 | Loss: 0.4211 | Acc: 0.8772\n",
      "Iter  800 | Loss: 0.4194 | Acc: 0.8777\n",
      "Iter  810 | Loss: 0.4178 | Acc: 0.8783\n",
      "Iter  820 | Loss: 0.4161 | Acc: 0.8788\n",
      "Iter  830 | Loss: 0.4146 | Acc: 0.8792\n",
      "Iter  840 | Loss: 0.4130 | Acc: 0.8797\n",
      "Iter  850 | Loss: 0.4115 | Acc: 0.8801\n",
      "Iter  860 | Loss: 0.4100 | Acc: 0.8805\n",
      "Iter  870 | Loss: 0.4086 | Acc: 0.8809\n",
      "Iter  880 | Loss: 0.4072 | Acc: 0.8815\n",
      "Iter  890 | Loss: 0.4058 | Acc: 0.8820\n",
      "Iter  900 | Loss: 0.4044 | Acc: 0.8824\n",
      "Iter  910 | Loss: 0.4031 | Acc: 0.8827\n",
      "Iter  920 | Loss: 0.4017 | Acc: 0.8831\n",
      "Iter  930 | Loss: 0.4005 | Acc: 0.8833\n",
      "Iter  940 | Loss: 0.3992 | Acc: 0.8838\n",
      "Iter  950 | Loss: 0.3980 | Acc: 0.8840\n",
      "Iter  960 | Loss: 0.3968 | Acc: 0.8842\n",
      "Iter  970 | Loss: 0.3956 | Acc: 0.8847\n",
      "Iter  980 | Loss: 0.3944 | Acc: 0.8851\n",
      "Iter  990 | Loss: 0.3932 | Acc: 0.8855\n",
      "Iter 1000 | Loss: 0.3921 | Acc: 0.8858\n",
      "Iter 1010 | Loss: 0.3910 | Acc: 0.8862\n",
      "Iter 1020 | Loss: 0.3899 | Acc: 0.8863\n",
      "Iter 1030 | Loss: 0.3889 | Acc: 0.8867\n",
      "Iter 1040 | Loss: 0.3878 | Acc: 0.8870\n",
      "Iter 1050 | Loss: 0.3868 | Acc: 0.8874\n",
      "Iter 1060 | Loss: 0.3857 | Acc: 0.8877\n",
      "Iter 1070 | Loss: 0.3847 | Acc: 0.8880\n",
      "Iter 1080 | Loss: 0.3837 | Acc: 0.8883\n",
      "Iter 1090 | Loss: 0.3828 | Acc: 0.8887\n",
      "Iter 1100 | Loss: 0.3818 | Acc: 0.8890\n",
      "Iter 1110 | Loss: 0.3809 | Acc: 0.8894\n",
      "Iter 1120 | Loss: 0.3799 | Acc: 0.8896\n",
      "Iter 1130 | Loss: 0.3790 | Acc: 0.8899\n",
      "Iter 1140 | Loss: 0.3781 | Acc: 0.8902\n",
      "Iter 1150 | Loss: 0.3772 | Acc: 0.8904\n",
      "Iter 1160 | Loss: 0.3764 | Acc: 0.8907\n",
      "Iter 1170 | Loss: 0.3755 | Acc: 0.8909\n",
      "Iter 1180 | Loss: 0.3746 | Acc: 0.8912\n",
      "Iter 1190 | Loss: 0.3738 | Acc: 0.8915\n",
      "Iter 1200 | Loss: 0.3730 | Acc: 0.8917\n",
      "Iter 1210 | Loss: 0.3722 | Acc: 0.8921\n",
      "Iter 1220 | Loss: 0.3714 | Acc: 0.8923\n",
      "Iter 1230 | Loss: 0.3706 | Acc: 0.8925\n",
      "Iter 1240 | Loss: 0.3698 | Acc: 0.8928\n",
      "Iter 1250 | Loss: 0.3690 | Acc: 0.8931\n",
      "Iter 1260 | Loss: 0.3683 | Acc: 0.8934\n",
      "Iter 1270 | Loss: 0.3675 | Acc: 0.8936\n",
      "Iter 1280 | Loss: 0.3668 | Acc: 0.8938\n",
      "Iter 1290 | Loss: 0.3660 | Acc: 0.8940\n",
      "Iter 1300 | Loss: 0.3653 | Acc: 0.8942\n",
      "Iter 1310 | Loss: 0.3646 | Acc: 0.8945\n",
      "Iter 1320 | Loss: 0.3639 | Acc: 0.8947\n",
      "Iter 1330 | Loss: 0.3632 | Acc: 0.8950\n",
      "Iter 1340 | Loss: 0.3625 | Acc: 0.8950\n",
      "Iter 1350 | Loss: 0.3618 | Acc: 0.8951\n",
      "Iter 1360 | Loss: 0.3611 | Acc: 0.8953\n",
      "Iter 1370 | Loss: 0.3604 | Acc: 0.8955\n",
      "Iter 1380 | Loss: 0.3598 | Acc: 0.8959\n",
      "Iter 1390 | Loss: 0.3591 | Acc: 0.8960\n",
      "Iter 1400 | Loss: 0.3585 | Acc: 0.8962\n",
      "Iter 1410 | Loss: 0.3579 | Acc: 0.8963\n",
      "Iter 1420 | Loss: 0.3572 | Acc: 0.8966\n",
      "Iter 1430 | Loss: 0.3566 | Acc: 0.8969\n",
      "Iter 1440 | Loss: 0.3560 | Acc: 0.8972\n",
      "Iter 1450 | Loss: 0.3554 | Acc: 0.8973\n",
      "Iter 1460 | Loss: 0.3548 | Acc: 0.8975\n",
      "Iter 1470 | Loss: 0.3542 | Acc: 0.8976\n",
      "Iter 1480 | Loss: 0.3536 | Acc: 0.8980\n",
      "Iter 1490 | Loss: 0.3530 | Acc: 0.8981\n",
      "Iter 1500 | Loss: 0.3524 | Acc: 0.8982\n",
      "Iter 1510 | Loss: 0.3518 | Acc: 0.8983\n",
      "Iter 1520 | Loss: 0.3513 | Acc: 0.8986\n",
      "Iter 1530 | Loss: 0.3507 | Acc: 0.8988\n",
      "Iter 1540 | Loss: 0.3501 | Acc: 0.8990\n",
      "Iter 1550 | Loss: 0.3496 | Acc: 0.8992\n",
      "Iter 1560 | Loss: 0.3490 | Acc: 0.8993\n",
      "Iter 1570 | Loss: 0.3485 | Acc: 0.8995\n",
      "Iter 1580 | Loss: 0.3480 | Acc: 0.8997\n",
      "Iter 1590 | Loss: 0.3474 | Acc: 0.8999\n",
      "Iter 1600 | Loss: 0.3469 | Acc: 0.9001\n",
      "Iter 1610 | Loss: 0.3464 | Acc: 0.9003\n",
      "Iter 1620 | Loss: 0.3458 | Acc: 0.9004\n",
      "Iter 1630 | Loss: 0.3453 | Acc: 0.9006\n",
      "Iter 1640 | Loss: 0.3448 | Acc: 0.9007\n",
      "Iter 1650 | Loss: 0.3443 | Acc: 0.9009\n",
      "Iter 1660 | Loss: 0.3438 | Acc: 0.9010\n",
      "Iter 1670 | Loss: 0.3433 | Acc: 0.9011\n",
      "Iter 1680 | Loss: 0.3428 | Acc: 0.9011\n",
      "Iter 1690 | Loss: 0.3423 | Acc: 0.9012\n",
      "Iter 1700 | Loss: 0.3418 | Acc: 0.9012\n",
      "Iter 1710 | Loss: 0.3413 | Acc: 0.9014\n",
      "Iter 1720 | Loss: 0.3408 | Acc: 0.9015\n",
      "Iter 1730 | Loss: 0.3404 | Acc: 0.9016\n",
      "Iter 1740 | Loss: 0.3399 | Acc: 0.9017\n",
      "Iter 1750 | Loss: 0.3394 | Acc: 0.9018\n",
      "Iter 1760 | Loss: 0.3390 | Acc: 0.9018\n",
      "Iter 1770 | Loss: 0.3385 | Acc: 0.9019\n",
      "Iter 1780 | Loss: 0.3380 | Acc: 0.9020\n",
      "Iter 1790 | Loss: 0.3376 | Acc: 0.9021\n",
      "Iter 1800 | Loss: 0.3371 | Acc: 0.9022\n",
      "Iter 1810 | Loss: 0.3367 | Acc: 0.9024\n",
      "Iter 1820 | Loss: 0.3362 | Acc: 0.9024\n",
      "Iter 1830 | Loss: 0.3358 | Acc: 0.9026\n",
      "Iter 1840 | Loss: 0.3354 | Acc: 0.9028\n",
      "Iter 1850 | Loss: 0.3349 | Acc: 0.9029\n",
      "Iter 1860 | Loss: 0.3345 | Acc: 0.9030\n",
      "Iter 1870 | Loss: 0.3341 | Acc: 0.9032\n",
      "Iter 1880 | Loss: 0.3336 | Acc: 0.9033\n",
      "Iter 1890 | Loss: 0.3332 | Acc: 0.9035\n",
      "Iter 1900 | Loss: 0.3328 | Acc: 0.9036\n",
      "Iter 1910 | Loss: 0.3324 | Acc: 0.9038\n",
      "Iter 1920 | Loss: 0.3320 | Acc: 0.9039\n",
      "Iter 1930 | Loss: 0.3316 | Acc: 0.9042\n",
      "Iter 1940 | Loss: 0.3312 | Acc: 0.9043\n",
      "Iter 1950 | Loss: 0.3308 | Acc: 0.9044\n",
      "Iter 1960 | Loss: 0.3304 | Acc: 0.9046\n",
      "Iter 1970 | Loss: 0.3300 | Acc: 0.9047\n",
      "Iter 1980 | Loss: 0.3296 | Acc: 0.9048\n",
      "Iter 1990 | Loss: 0.3292 | Acc: 0.9050\n"
     ]
    }
   ],
   "source": [
    "# Train the neural network using gradient descent\n",
    "# X_train and Y_train are the training data and labels\n",
    "# 2000 is the number of training iterations\n",
    "# 0.1 is the learning rate that controls how much the weights are updated each step\n",
    "# The function returns the learned weights and biases after training\n",
    "W1, b1, W2, b2 = gradient_descent(X_train, Y_train, 2000, 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ae362219-3d07-465c-b4d9-4b22cfd4a218",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_predictions(X, W1, b1, W2, b2):\n",
    "    # Perform forward propagation using the trained weights and biases\n",
    "    # This computes the final output probabilities (A2)\n",
    "    _, _, _, A2 = forward_prop(W1, b1, W2, b2, X)\n",
    "    \n",
    "    # Convert the output probabilities into predicted class labels\n",
    "    # The class with the highest probability is chosen as the prediction\n",
    "    predictions = get_predictions(A2)\n",
    "    \n",
    "    # Return the predicted labels\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5c87fd75-ec8f-4ef0-a61b-a949dfb573b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_prediction(index, W1, b1, W2, b2, X, Y):\n",
    "    # Select a single image from the dataset using the given index\n",
    "    # The image is kept as a column vector to match the network input shape\n",
    "    current_image = X[:, index, None]\n",
    "\n",
    "    # Make a prediction for the selected image using the trained model\n",
    "    prediction = make_predictions(current_image, W1, b1, W2, b2)\n",
    "\n",
    "    # Extract the predicted label and convert it to an integer\n",
    "    pred_label = int(prediction[0])\n",
    "    \n",
    "    # Get the true label for comparison\n",
    "    true_label = int(Y[index])\n",
    "\n",
    "    # Print the predicted label and the true label\n",
    "    print(\"Prediction:\", pred_label)\n",
    "    print(\"Label:\", true_label)\n",
    "\n",
    "    # Reshape the image back to 28x28 format for visualization\n",
    "    # Multiply by 255 to restore original pixel values\n",
    "    image_display = current_image.reshape(28, 28) * 255\n",
    "\n",
    "    # Display the image in grayscale\n",
    "    plt.imshow(image_display, cmap=\"gray\")\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9c96e799-55ea-449b-97dd-930441e076a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: 7\n",
      "Label: 7\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: 7\n",
      "Label: 7\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: 6\n",
      "Label: 6\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAGFCAYAAAASI+9IAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjgsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvwVt1zgAAAAlwSFlzAAAPYQAAD2EBqD+naQAACAFJREFUeJzt3L+rVnUAx/Fz5Eo/BIOIRh0dbAqCWoK2bHF0TAiCgkCCcIoQWoIgogiitlqCFgcR3C7+AxmCS06JOV/UgsDT1Nuh5fke7OF6fb2mZ3g+PHK8977vV/A7L8uyTAAwTdMhTwGAf4kCABEFACIKAEQUAIgoABBRACCiAEB2pg3N87zpWwHYhzb5v8pOCgBEFACIKAAQUQAgogBARAGAiAIAEQUAIgoARBQAiCgAEFEAIKIAQEQBgIgCABEFACIKAEQUAIgoABBRACCiAEBEAYCIAgARBQAiCgBEFACIKAAQUQAgogBARAGAiAIAEQUARAGA/3JSACCiAEBEAYCIAgARBQAiCgBk5+FLeLK9/vrrw5uvv/56ePP0009Pa7z88svDm7t37676LJ5cTgoARBQAiCgAEFEAIKIAQEQBgIgCABEFACIKAEQUAIgoABBRACAuxONAevHFF4c3n3322fDm5MmTw5vff/99WuPQIb/D8f/zVQZARAGAiAIAEQUAIgoARBQAiCgAEFEAIKIAQEQBgIgCABEFAOJCPPa9I0eODG++/PLL4c0rr7wyvPnxxx+HNx988MG0xt7e3qodjHBSACCiAEBEAYCIAgARBQAiCgBEFACIKAAQUQAgogBARAGAiAIAmZdlWaYNzPO8ydvgkfv222+HN++8887w5vbt28ObN954Y3hz8+bN4Q08Cpv8uHdSACCiAEBEAYCIAgARBQAiCgBEFACIKAAQUQAgogBARAGAiAIA2Xn4Evan06dPb+VzLl68OLxxuR0HjZMCABEFACIKAEQUAIgoABBRACCiAEBEAYCIAgARBQAiCgBEFACIKAAQt6SyNc8999yq3eHDh4c38zwPb65evTq8gYPGSQGAiAIAEQUAIgoARBQAiCgAEFEAIKIAQEQBgIgCABEFACIKAMSFeGzN+++/v7WL9H755ZfhzaVLl4Y3cNA4KQAQUQAgogBARAGAiAIAEQUAIgoARBQAiCgAEFEAIKIAQEQBgLgQj1V2dsa/dN56662tPe0vvvhieHPv3r3/5c8CjxMnBQAiCgBEFACIKAAQUQAgogBARAGAiAIAEQUAIgoARBQAiCgAEBfiscqJEyeGN6+99trWnvZff/21tc+Cg8RJAYCIAgARBQAiCgBEFACIKAAQUQAgogBARAGAiAIAEQUAIgoARBQAiCgAEFEAIKIAQEQBgIgCABEFACIKAEQUAIgoABBRACCiAEBEAYCIAgCZl2VZpg3M87zJ23hCnDx5cnjz66+/Ttty/fr14c1LL700vDl0aPz3qgcPHkxrfPXVV8Obc+fOrfosDqZNftw7KQAQUQAgogBARAGAiAIAEQUAIgoARBQAiCgAEFEAIKIAQEQBgLgQj61diHft2rUD97TXXBS54R2U//H3338Pb958883hze7u7vCGx4ML8QAY4p+PAIgoABBRACCiAEBEAYCIAgARBQAiCgBEFACIKAAQUQAgOw9fwuaeeeaZff249vb2hjcfffTR8Obq1avDm8uXL09rHD9+fHhz7NixVZ/Fk8tJAYCIAgARBQAiCgBEFACIKAAQUQAgogBARAGAiAIAEQUAIgoAxIV4rPL222/v6yf37rvvDm9+/vnnaRu+++67VbtPP/10eHP27NnhzQ8//DC84eBwUgAgogBARAGAiAIAEQUAIgoARBQAiCgAEFEAIKIAQEQBgIgCABEFAOKWVFb56aefhjfvvfeepz1N082bNz0H9i0nBQAiCgBEFACIKAAQUQAgogBARAGAiAIAEQUAIgoARBQAiCgAEBfiscqdO3eGN3/++eeqz3r22Weng+T5559ftZvneSsbnmxOCgBEFACIKAAQUQAgogBARAGAiAIAEQUAIgoARBQAiCgAEFEAIC7EY5XffvttePP555+v+qzz588Pb1599dXhzZUrV4Y3e3t7w5szZ85MayzLMrx54YUXhjdHjx7dynNgf3JSACCiAEBEAYCIAgARBQAiCgBEFACIKAAQUQAgogBARAGAiAIAcSEeW3PhwoVVu7Nnzw5vzp07N7x56qmnhje7u7vDm+PHj0/b8scffwxv7t+//7/8WXg8OCkAEFEAIKIAQEQBgIgCABEFACIKAEQUAIgoABBRACCiAEBEAYDMy7Is0wbmed7kbfDIffLJJ8Objz/+eCt/E2u+Lzb8lvuPGzduDG9OnTo1vLl169bwhsfDJl97TgoARBQAiCgAEFEAIKIAQEQBgIgCABEFACIKAEQUAIgoABBRACCiAEB2Hr6E/embb74Z3hw5cmR48+GHH07bcPHixVW777//fnjjxlNGOSkAEFEAIKIAQEQBgIgCABEFACIKAEQUAIgoABBRACCiAEBEAYDMy7Is0wbmed7kbQDsU5v8uHdSACCiAEBEAYCIAgARBQAiCgBEFACIKAAQUQAgogBARAGAiAIAEQUAIgoARBQAiCgAEFEAIKIAQEQBgIgCABEFACIKAEQUAIgoABBRACCiAEBEAYCIAgARBQAiCgBEFACIKAAQUQAgogBARAGAiAIAEQUAIgoARBQAiCgAEFEAIKIAQEQBgIgCABEFACIKAEQUAIgoABBRACCiAEBEAYCIAgARBQAiCgBEFACIKAAQUQAgogBARAGAiAIAEQUAIgoARBQAiCgAEFEAIDvThpZl2fStADymnBQAiCgAEFEAIKIAQEQBgIgCABEFACIKAEQUAJj+9Q/HsNCwvPeY6wAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: 7\n",
      "Label: 7\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: 8\n",
      "Label: 8\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Loop through the first 5 training examples\n",
    "# For each example, display the image and show the model's prediction\n",
    "# This helps visually check how well the model is performing on training data\n",
    "for i in range(5):\n",
    "    test_prediction(i, W1, b1, W2, b2, X_train, Y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da049d35-d15f-4bb9-81bc-81d6ad8d1817",
   "metadata": {},
   "source": [
    "# Evaluating the Model on Development Data\n",
    "\n",
    "## Making Predictions on Dev Set\n",
    "- Use the trained model (W1, b1, W2, b2) to predict labels for all development examples (X_dev).  \n",
    "- The predicted labels are stored in `dev_predictions`.  \n",
    "- Calculate the accuracy by comparing predictions with the true labels (Y_dev) using `get_accuracy`.  \n",
    "- Print the overall development set accuracy to see how well the model performs on unseen data.\n",
    "\n",
    "---\n",
    "\n",
    "## Visualizing Single Dev Examples\n",
    "- The function `show_dev_prediction` allows us to inspect individual predictions visually.  \n",
    "- Steps:\n",
    "  1. Extract a single image from the development set using the given index.  \n",
    "  2. Make a prediction using the trained model.  \n",
    "  3. Print both the predicted label and the true label.  \n",
    "  4. Display the image in grayscale to see what the model is predicting.\n",
    "\n",
    "---\n",
    "\n",
    "## Sample Predictions from Dev Set\n",
    "- Display predictions for a few selected examples from the development set.  \n",
    "- Helps check how well the model generalizes to data it has not seen during training.  \n",
    "- Examples include indices 0, 12, and 57, showing both the predicted label and the true label along with the corresponding image.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e137add8-0e4a-4bc1-91d6-53a35b1b609e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dev accuracy: 0.9085\n"
     ]
    }
   ],
   "source": [
    "dev_predictions = make_predictions(X_dev, W1, b1, W2, b2)\n",
    "dev_acc = get_accuracy(dev_predictions, Y_dev)\n",
    "print(\"Dev accuracy:\", dev_acc)\n",
    "\n",
    "def show_dev_prediction(index, W1, b1, W2, b2, X_dev, Y_dev):\n",
    "    image = X_dev[:, index, None]\n",
    "    prediction = make_predictions(image, W1, b1, W2, b2)[0]\n",
    "    label = Y_dev[index]\n",
    "\n",
    "    print(f\"Prediction: {prediction}\")\n",
    "    print(f\"Label: {label}\")\n",
    "\n",
    "    plt.imshow(image.reshape(28, 28), cmap=\"gray\")\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "072a868f-0b93-4feb-a10d-7d1d390f6780",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: 7\n",
      "Label: 7\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: 9\n",
      "Label: 9\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: 1\n",
      "Label: 1\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAGFCAYAAAASI+9IAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjgsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvwVt1zgAAAAlwSFlzAAAPYQAAD2EBqD+naQAABvpJREFUeJzt3D9qFWsYwOEzElIELUUQrAIuQC3chCA2bsDCZcQF2GuRJdjoEmzdgQGtFC20UQkBmdtcf4HrLeY7xGP+PE81xbzMcAjz8y38pnme5xUArFarS34FAH4RBQAiCgBEFACIKAAQUQAgogBARAGAbK0WmqZp6a0AnEJL/q+yTQGAiAIAEQUAIgoARBQAiCgAEFEAIKIAQEQBgIgCABEFACIKAEQUAIgoABBRACCiAEBEAYCIAgARBQAiCgBEFACIKAAQUQAgogBARAGAiAIAEQUAIgoARBQAiCgAEFEAIKIAgCgA8DubAgARBQAiCgBEFACIKAAQUQAgogBARAGAiAIAEQUAIgoARBQAiCgAEFEAIKIAQEQBgIgCABEFACIKAEQUABAFAH5nUwAgogBARAGAiAIAEQUAIgoARBQAiCgAEFEAIKIAQEQBgGwdXwKj7t27Nzzz8uXLtX7ox48fD888f/58rWdxcdkUAIgoABBRACCiAEBEAYCIAgARBQAiCgBEFACIKAAQUQAgogBApnme59UC0zQtuQ0ulDdv3gzP3Lp1a61nHRwcDM/cvHlzrWdxPi353NsUAIgoABBRACCiAEBEAYCIAgARBQAiCgBEFACIKAAQUQAgogBAto4vgVG3b98enll4BuVvvnz5stYcjLApABBRACCiAEBEAYCIAgARBQAiCgBEFACIKAAQUQAgogBARAGAiAIAcUoqnBHPnj3726/ABWBTACCiAEBEAYCIAgARBQAiCgBEFACIKAAQUQAgogBARAGAiAIAcSAebNjR0dFac58+fTrxd4H/sikAEFEAIKIAQEQBgIgCABEFACIKAEQUAIgoABBRACCiAEBEAYBM8zzPqwWmaVpyG5xZu7u7wzMHBwfDMx8/flyt4/r162vNwS9LPvc2BQAiCgBEFACIKAAQUQAgogBARAGAiAIAEQUAIgoARBQAiCgAkK3jS7jY9vb2NvKc/f39jTwH1mFTACCiAEBEAYCIAgARBQAiCgBEFACIKAAQUQAgogBARAGAiAIAEQUA4pRU+Nf9+/c38lt8/vzZb86pZVMAIKIAQEQBgIgCABEFACIKAEQUAIgoABBRACCiAEBEAYCIAgBxIB7n0qNHj4ZndnZ2hme+ffs2PLO/vz88A5tiUwAgogBARAGAiAIAEQUAIgoARBQAiCgAEFEAIKIAQEQBgIgCAHEgHufSOofbTdM0PPP06dPhmR8/fgzPwKbYFACIKAAQUQAgogBARAGAiAIAEQUAIgoARBQAiCgAEFEAIKIAQByIx7m0vb29kQPx9vb2hmfgNLMpABBRACCiAEBEAYCIAgARBQAiCgBEFACIKAAQUQAgogBARAGATPM8z6s/dFgYnISdnZ3hmXfv3g3PXL16dXjm0iX/ruLsWPK59xcNQEQBgIgCABEFACIKAEQUAIgoABBRACCiAEBEAYCIAgARBQAiCgBk6/gSTqeHDx9u5MTTr1+/Ds/AeWNTACCiAEBEAYCIAgARBQAiCgBEFACIKAAQUQAgogBARAGAiAIAcSAep96DBw828pwnT55s5DlwmtkUAIgoABBRACCiAEBEAYCIAgARBQAiCgBEFACIKAAQUQAgogBApnme59UC0zQtuQ1O3Nu3b4dndnd3h2euXLkyPPP9+/fhGfhblnzubQoARBQAiCgAEFEAIKIAQEQBgIgCABEFACIKAEQUAIgoABBRACBbx5dwfrx48WJ45vDw8I+8C5wlNgUAIgoARBQAiCgAEFEAIKIAQEQBgIgCABEFACIKAEQUAIgoABAH4rExN27cWGvu8uXLwzMfPnwYnvn58+fwDJw3NgUAIgoARBQAiCgAEFEAIKIAQEQBgIgCABEFACIKAEQUAIgoABBRACBOSWVj7t69u9bctWvXTvxdgP9nUwAgogBARAGAiAIAEQUAIgoARBQAiCgAEFEAIKIAQEQBgIgCAHEgHhvz6tWrtebev39/4u8C/D+bAgARBQAiCgBEFACIKAAQUQAgogBARAGAiAIAEQUAIgoARBQAiAPx2JjDw8O15l6/fj08c+fOneGZ7e3t4Zmjo6PhGTjNbAoARBQAiCgAEFEAIKIAQEQBgIgCABEFACIKAEQUAIgoABBRACDTPM/zaoFpmpbcBsApteRzb1MAIKIAQEQBgIgCABEFACIKAEQUAIgoABBRACCiAEBEAYCIAgARBQAiCgBEFACIKAAQUQAgogBARAGAiAIAEQUAIgoARBQAiCgAEFEAIKIAQEQBgIgCABEFACIKAEQUAIgoABBRACCiAEBEAYCIAgARBQAiCgBEFADI1mqheZ6X3grAGWVTACCiAEBEAYCIAgARBQAiCgBEFACIKAAQUQBg9cs/NpyMMBLbldgAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_dev_prediction(0, W1, b1, W2, b2, X_dev, Y_dev)\n",
    "show_dev_prediction(12, W1, b1, W2, b2, X_dev, Y_dev)\n",
    "show_dev_prediction(57, W1, b1, W2, b2, X_dev, Y_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7e99871-34f4-422f-bc72-eb88820a537b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82d5538d-2de7-4430-b7f9-91aaf5a7de27",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
